{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821b066b-830a-47c1-8325-074cd906b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# PIL (Python Imaging Library) is used here via its 'Image' module to load and process images\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch is a deep learning framework. The 'torch' base module provides core functionality like tensors and GPU acceleration\n",
    "import torch\n",
    "\n",
    "# 'nn' provides tools for building neural network layers and models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 'F' contains functional versions of neural network layers and common functions like activation functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 'optim' provides optimization algorithms like SGD, Adam, etc., used for training models\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision is a PyTorch library that provides datasets, model architectures, and image transformations\n",
    "import torchvision\n",
    "\n",
    "# 'transforms' provides common image preprocessing operations like resizing, normalization, and converting images to tensors\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed0b9c8-02ae-4445-be40-c14e977eb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transform with data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),           # Randomly flip the image horizontally with a 50% chance.\n",
    "    transforms.RandomCrop(32, padding=4),        # Randomly crop the image to 32x32 with up to 4 pixels of padding on each side.\n",
    "    transforms.ToTensor(),                       # Convert the image from PIL format to a PyTorch tensor.\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),        # Normalize the image: subtract mean and divide by std for each channel (R, G, B).\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Testing transform without augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                       # Convert the image from PIL format to a PyTorch tensor.\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),        # Normalize the image using the same mean and std as training.\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22cc83f-5aa9-4ee6-874c-6c37c5a7947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 training dataset\n",
    "# - root: directory where the dataset will be saved or loaded from\n",
    "# - train=True: load the training data split\n",
    "# - transform: preprocessing to apply to each image (e.g., ToTensor, normalization, etc.)\n",
    "# - download=True: download the dataset if it's not already present\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root = r\"C:\\Users\\isaia\\Documents\\ImageClassification_project\",\n",
    "    train = True,\n",
    "    transform = train_transform,\n",
    "    download = True\n",
    ")\n",
    "\n",
    "# Load the CIFAR-10 test dataset (same settings, but with train=False)\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root = r\"C:\\Users\\isaia\\Documents\\ImageClassification_project\",\n",
    "    train = False,\n",
    "    transform = test_transform,\n",
    "    download = True\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "# - batch_size=32: feed data in mini-batches of 32 images\n",
    "# - shuffle=True: shuffle the data at each epoch to improve learning\n",
    "# - num_workers=2: use 2 worker processes to load data in parallel for better performance\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test dataset with the same settings\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86c22da3-aa8d-4243-842b-7c71f6e327a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the first sample from the training dataset:\n",
    "# 'image' gets the transformed image tensor (after applying the defined transforms like ToTensor and Normalize)\n",
    "# 'label' gets the corresponding integer class label for that image\n",
    "image, label = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3baca2de-d67c-4142-80d2-685a48d5b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size() # 3 channels (RGB), 32 x 32 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "072efa6c-a7d0-4ff0-b0b3-4436f141ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"] # 0 - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15be9cfc-2978-4853-8cba-06be0c52e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers are used to extract spatial features from images.\n",
    "        # They slide filters (small weight matrices) over the image to detect patterns like edges, textures, and shapes.\n",
    "\n",
    "        # 1st convolutional layer:\n",
    "        # - Takes in 3-channel (RGB) images\n",
    "        # - Learns 12 filters of size 5x5\n",
    "        # Each filter outputs a 2D feature map; the result is 12 such maps → shape (out_channels, output_height, output_width)\n",
    "        # formula: output_size = (input_size - kernel_size) / stride + 1\n",
    "        # (32 - 5) / 1 + 1 = 28\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5) # Output: (12, 28, 28)\n",
    "\n",
    "        # Pooling layer:\n",
    "        # - Reduces the spatial size by taking the max value in each 2x2 window\n",
    "        # - Helps reduce computation and makes features more robust to small shifts\n",
    "        # We'll reuse this layer after each conv layer\n",
    "        # (28 - 2) / 2 + 1 = 14\n",
    "        self.pool = nn.MaxPool2d(2, 2) # e.g., (12, 28, 28) → (12, 14, 14)\n",
    "\n",
    "        # 2nd convolutional layer:\n",
    "        # - Input: 12 channels\n",
    "        # - Learns 24 new filters of size 5x5\n",
    "        # - Further extracts higher-level features from previous feature maps\n",
    "        # Output: (24, 10, 10), then pooling → (24, 5, 5)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5) # Output before pooling: (24, 10, 10)\n",
    "\n",
    "        # 3rd convolutional layer:\n",
    "        # - Input: 24 channels\n",
    "        # - Learns 48 filters of size 3x3\n",
    "        # - More compact filter to detect fine details and deeper features\n",
    "        # Output: (48, 3, 3), then pooling → (48, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(24, 48, 3) # Output before pooling: (48, 3, 3)\n",
    "\n",
    "        # Dropout layer:\n",
    "        # - Randomly zeroes some of the elements of the input tensor with probability p\n",
    "        # - Helps prevent overfitting by adding noise during training\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Fully connected layers:\n",
    "        # - After the conv and pool layers, we flatten the output tensor into a vector\n",
    "        # - These layers function like a traditional neural network, learning how to map features to class scores\n",
    "\n",
    "        # fc1: Takes in flattened features (48 feature maps × 1 × 1 pixels) and outputs 120 values\n",
    "        self.fc1 = nn.Linear(48 * 1 * 1, 120)\n",
    "\n",
    "        # fc2: Further reduces to 84 values\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # fc3: Final output layer\n",
    "        # - Outputs 10 scores (logits), one for each class in CIFAR-10\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # how the data will flow through the neural network\n",
    "    def forward(self, x):\n",
    "        # Pass input x through the first convolutional layer (self.conv1)\n",
    "        # conv1 applies 12 filters (kernels) of size 5x5 to the input image,\n",
    "        # producing 12 feature maps of size 28x28 (assuming input is 32x32 RGB)\n",
    "\n",
    "        # Apply ReLU activation to introduce non-linearity\n",
    "        # ReLU replaces negative values with 0 to help the model learn complex patterns\n",
    "\n",
    "        # Apply 2x2 max pooling with stride 2 to reduce spatial size\n",
    "        # This halves the height and width: (12, 28, 28) → (12, 14, 14)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # → (12, 14, 14)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x))) # → (24, 5, 5)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv3(x))) # → (48, 1, 1)\n",
    "\n",
    "        x = torch.flatten(x, 1) # Flatten the tensor starting from dimension 1 (keep batch dimension intact)\n",
    "        # Suppose x has shape (batch_size, 48, 1, 1) after convolution and pooling\n",
    "        # This reshapes each image to a 1D vector of length 48 (48*1*1)\n",
    "        # Final shape becomes (batch_size, 48), ready for the fully connected (linear) layers\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x))) # Apply dropout after fc1 to reduce overfitting\n",
    "\n",
    "        x = F.relu(self.fc2(x)) # Pass through the second fully connected layer (fc2), then apply ReLU\n",
    "\n",
    "        x = self.fc3(x) # Final fully connected layer outputs raw scores (logits) for each of the 10 classes\n",
    "\n",
    "        return x # Return the output scores (logits) for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a257225a-f373-4e50-baa5-2310ab1f1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet()  # Create an instance of the NeuralNet model, initializing all layers\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "# Define the loss function to measure how well the model’s predictions match the true labels\n",
    "# CrossEntropyLoss is suitable for multi-class classification tasks like CIFAR-10\n",
    "\n",
    "# Create an Adam optimizer to update the model's parameters during training\n",
    "# - model.parameters(): gets all learnable parameters (weights & biases) in the model\n",
    "# - lr=0.001: sets the learning rate, controlling how much to update the parameters at each step\n",
    "# - torch.optim.Adam: uses the Adam optimization algorithm (adaptive moment estimation), which combines ideas from momentum and RMSProp for efficient training\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)  \n",
    "# Set up the optimizer to update model parameters using Stochastic Gradient Descent (SGD)\n",
    "# net.parameters() passes all learnable parameters (weights and biases) to the optimizer\n",
    "# lr=0.001 sets the learning rate (step size for updates)\n",
    "# momentum=0.9 helps accelerate training by smoothing updates and reducing oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db8491a1-52ab-4d82-85a4-bfeb4c8a1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the training loop for the neural network.\n",
    "# It runs for 30 epochs, meaning the model sees the full dataset 30 times.\n",
    "# For each epoch, we loop over mini-batches of training data using train_loader.\n",
    "# Inside the loop, we unpack each batch into inputs (images) and labels (true classes).\n",
    "# We reset the gradients accumulated from the previous batch to zero before each update using optimizer.zero_grad().\n",
    "# The model makes predictions on the inputs using net(inputs).\n",
    "# We compute the loss between the predictions and the actual labels using CrossEntropyLoss.\n",
    "# We then perform backpropagation with loss.backward() to compute gradients.\n",
    "# optimizer.step() updates the model's weights using the gradients and learning rate.\n",
    "# We accumulate the loss values over the epoch using running_loss.\n",
    "# At the end of the epoch, we print the average loss over all batches to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ace6353-a181-453b-8221-a8e0e5634f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0...\n",
      "Loss: 1.8398\n",
      "Training epoch 1...\n",
      "Loss: 1.5715\n",
      "Training epoch 2...\n",
      "Loss: 1.4620\n",
      "Training epoch 3...\n",
      "Loss: 1.3846\n",
      "Training epoch 4...\n",
      "Loss: 1.3343\n",
      "Training epoch 5...\n",
      "Loss: 1.2811\n",
      "Training epoch 6...\n",
      "Loss: 1.2594\n",
      "Training epoch 7...\n",
      "Loss: 1.2350\n",
      "Training epoch 8...\n",
      "Loss: 1.2077\n",
      "Training epoch 9...\n",
      "Loss: 1.1928\n",
      "Training epoch 10...\n",
      "Loss: 1.1734\n",
      "Training epoch 11...\n",
      "Loss: 1.1668\n",
      "Training epoch 12...\n",
      "Loss: 1.1519\n",
      "Training epoch 13...\n",
      "Loss: 1.1413\n",
      "Training epoch 14...\n",
      "Loss: 1.1385\n",
      "Training epoch 15...\n",
      "Loss: 1.1212\n",
      "Training epoch 16...\n",
      "Loss: 1.1177\n",
      "Training epoch 17...\n",
      "Loss: 1.1140\n",
      "Training epoch 18...\n",
      "Loss: 1.1049\n",
      "Training epoch 19...\n",
      "Loss: 1.0942\n",
      "Training epoch 20...\n",
      "Loss: 1.0946\n",
      "Training epoch 21...\n",
      "Loss: 1.0868\n",
      "Training epoch 22...\n",
      "Loss: 1.0814\n",
      "Training epoch 23...\n",
      "Loss: 1.0783\n",
      "Training epoch 24...\n",
      "Loss: 1.0748\n",
      "Training epoch 25...\n",
      "Loss: 1.0727\n",
      "Training epoch 26...\n",
      "Loss: 1.0619\n",
      "Training epoch 27...\n",
      "Loss: 1.0593\n",
      "Training epoch 28...\n",
      "Loss: 1.0582\n",
      "Training epoch 29...\n",
      "Loss: 1.0496\n",
      "Training epoch 30...\n",
      "Loss: 1.0495\n",
      "Training epoch 31...\n",
      "Loss: 1.0431\n",
      "Training epoch 32...\n",
      "Loss: 1.0439\n",
      "Training epoch 33...\n",
      "Loss: 1.0419\n",
      "Training epoch 34...\n",
      "Loss: 1.0364\n",
      "Training epoch 35...\n",
      "Loss: 1.0333\n",
      "Training epoch 36...\n",
      "Loss: 1.0335\n",
      "Training epoch 37...\n",
      "Loss: 1.0310\n",
      "Training epoch 38...\n",
      "Loss: 1.0300\n",
      "Training epoch 39...\n",
      "Loss: 1.0331\n",
      "Training epoch 40...\n",
      "Loss: 1.0238\n",
      "Training epoch 41...\n",
      "Loss: 1.0228\n",
      "Training epoch 42...\n",
      "Loss: 1.0215\n",
      "Training epoch 43...\n",
      "Loss: 1.0161\n",
      "Training epoch 44...\n",
      "Loss: 1.0175\n",
      "Training epoch 45...\n",
      "Loss: 1.0121\n",
      "Training epoch 46...\n",
      "Loss: 1.0173\n",
      "Training epoch 47...\n",
      "Loss: 1.0101\n",
      "Training epoch 48...\n",
      "Loss: 1.0085\n",
      "Training epoch 49...\n",
      "Loss: 1.0047\n",
      "Training epoch 50...\n",
      "Loss: 1.0078\n",
      "Training epoch 51...\n",
      "Loss: 1.0040\n",
      "Training epoch 52...\n",
      "Loss: 1.0086\n",
      "Training epoch 53...\n",
      "Loss: 1.0021\n",
      "Training epoch 54...\n",
      "Loss: 0.9971\n",
      "Training epoch 55...\n",
      "Loss: 0.9994\n",
      "Training epoch 56...\n",
      "Loss: 0.9953\n",
      "Training epoch 57...\n",
      "Loss: 0.9907\n",
      "Training epoch 58...\n",
      "Loss: 0.9943\n",
      "Training epoch 59...\n",
      "Loss: 0.9946\n",
      "Training epoch 60...\n",
      "Loss: 0.9901\n",
      "Training epoch 61...\n",
      "Loss: 0.9892\n",
      "Training epoch 62...\n",
      "Loss: 0.9870\n",
      "Training epoch 63...\n",
      "Loss: 0.9940\n",
      "Training epoch 64...\n",
      "Loss: 0.9882\n",
      "Training epoch 65...\n",
      "Loss: 0.9817\n",
      "Training epoch 66...\n",
      "Loss: 0.9823\n",
      "Training epoch 67...\n",
      "Loss: 0.9833\n",
      "Training epoch 68...\n",
      "Loss: 0.9859\n",
      "Training epoch 69...\n",
      "Loss: 0.9783\n",
      "Training epoch 70...\n",
      "Loss: 0.9782\n",
      "Training epoch 71...\n",
      "Loss: 0.9815\n",
      "Training epoch 72...\n",
      "Loss: 0.9774\n",
      "Training epoch 73...\n",
      "Loss: 0.9732\n",
      "Training epoch 74...\n",
      "Loss: 0.9683\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(75):\n",
    "    print(f\"Training epoch {epoch}...\")\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7aeb956-3610-4f91-9c85-30dd6a7b05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the trained model’s parameters (weights and biases) to a file called \"trained_net.pth\".\n",
    "torch.save(net.state_dict(), r\"C:\\Users\\isaia\\Documents\\ImageClassification_project\\trained_net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c5e37f3-9a66-4c45-b642-a3d627c6a218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "# loads the saved model\n",
    "net.load_state_dict(torch.load(r\"C:\\Users\\isaia\\Documents\\ImageClassification_project\\trained_net.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da26e4a1-c90f-408d-96e2-085411923333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.37%\n"
     ]
    }
   ],
   "source": [
    "correct = 0  # Total number of correct predictions\n",
    "total = 0    # Total number of samples tested\n",
    "\n",
    "net.eval()   # Set the model to evaluation mode (turn off dropout, etc.)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking for efficiency during inference\n",
    "    for data in test_loader:  # Loop through the test data in batches\n",
    "        images, labels = data  # Get input images and ground truth labels\n",
    "\n",
    "        outputs = net(images)  # Get model predictions (logits) for each class\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)  # Select class with highest score for each image\n",
    "\n",
    "        total += labels.size(0)  # Add batch size to total sample count\n",
    "\n",
    "        correct += (predicted == labels).sum().item()  # Count how many predictions were correct\n",
    "\n",
    "accuracy = 100 * correct / total  # Compute accuracy as a percentage\n",
    "\n",
    "print(f\"Accuracy: {accuracy}%\")  # Output final accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "199f312d-e2e4-40be-9c41-3be4120f6bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: dog\n",
      "Prediction: plane\n",
      "Prediction: deer\n",
      "Prediction: deer\n",
      "Prediction: bird\n",
      "Prediction: truck\n",
      "Prediction: bird\n",
      "Prediction: cat\n",
      "Prediction: car\n",
      "Prediction: frog\n"
     ]
    }
   ],
   "source": [
    "# Define a new transformation pipeline:\n",
    "# - Resize image to 32x32 pixels (CIFAR-10 input size)\n",
    "# - Convert the image to a PyTorch tensor\n",
    "# - Normalize RGB channels to range [-1, 1] using mean=0.5 and std=0.5\n",
    "new_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define a function to load and preprocess a single image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)        # Open image from disk\n",
    "    image = new_transform(image)          # Apply the defined transform\n",
    "    image = image.unsqueeze(0)            # Add batch dimension at 0 index → shape becomes [1, 3, 32, 32]\n",
    "    return image\n",
    "\n",
    "# List of image file paths to classify\n",
    "image_paths = [\n",
    "    r\"C:\\Users\\isaia\\Downloads\\dog.jpg\", \n",
    "    r\"C:\\Users\\isaia\\Downloads\\airplane.jpg\", \n",
    "    r\"C:\\Users\\isaia\\Downloads\\horse.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\deer.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\ship.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\truck.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\bird.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\cat.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\car.jpg\",\n",
    "    r\"C:\\Users\\isaia\\Downloads\\frog.jpg\",\n",
    "]\n",
    "# Preprocess each image and store in a list\n",
    "images = [load_image(img) for img in image_paths]\n",
    "\n",
    "net.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking (saves memory and computation)\n",
    "    for image in images:\n",
    "        output = net(image)                       # Forward pass through the network\n",
    "        _, predicted = torch.max(output, dim = 1)       # Get index of the highest scoring class, ignore the value, look across columns\n",
    "        print(f\"Prediction: {class_names[predicted.item()]}\")  # Print the class name of the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1ff3d-7bbd-48e6-a4dc-622e3cdb438e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
